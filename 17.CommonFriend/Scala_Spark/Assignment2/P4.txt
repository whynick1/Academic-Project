1. enter spark dirctory: cd /Users/wanghongyi/Applications/spark-2.0.0-bin-hadoop2.7/bin
2. execute spark: ./spark-shell

3. For problem2, run each line of P2.txt
4. For problem3, run each line of P3.txt
5. For problem4, run each line of P4.txt

Problem4import java.sql.Dateimport java.text.SimpleDateFormatimport org.apache.spark.sql.SQLContextimport org.apache.spark.SparkContext._val curDate = new Date(System.currentTimeMillis)def age(birthday: String) = {     val date=birthday.split("/")     val Month = curDate.getMonth()+1     val Year = curDate.getYear()+1900     var age = Year - date(2).toInt     if(date(0).toInt>Month){          age-= 1     }     else if(date(0).toInt==Month){          val currentDay=curDate.getDate();          if(date(1).toInt>currentDay) {               age-= 1          }     }     age.toFloat}val input = sc.textFile("/Users/wanghongyi/Applications/spark-2.0.0-bin-hadoop2.7/data/commonFriends/soc-LiveJournal1Adj.txt")val friends = input.map(li=>li.split("\\t")).filter(l1 => (l1.size == 2)).map(li=>(li(0),li(1).split(","))).flatMap(x=>x._2.flatMap(z=>Array((z,x._1))))val input2 = sc.textFile("/Users/wanghongyi/Applications/spark-2.0.0-bin-hadoop2.7/data/commonFriends/userdata.txt")val value1 = input2.map(li=>li.split(",")).map(li=>(li(0),age(li(9))))val value2 = friends.join(value1)def mean(Sum_Age: Iterable[Float]) = Sum_Age.maxval rdd = value2.groupBy(_._2._1).mapValues(Sum_Age=>mean(Sum_Age.map(_._2._2)))val ageArray = rdd.take(49124)val Sorted_Age = ageArray.sortBy(_._2).reverseval Final_20 = Sorted_Age.take(20)val Final_20sc = sc.parallelize(Final_20)val value3 = input2.map(li=>li.split(",")).map(li=>(li(0),li(1),li(3),li(4),li(5)))val key = value3.map({case(first,second,third,fourth,fifth) => first->(second,third,fourth,fifth)})val Join = Final_20sc.join(key).sortBy(_._2,false)val result = Join.map(x=>(x._2._2._1,x._2._2._2,x._2._2._3,x._2._2._4,x._2._1)).collect.mkString("\n").replace("(","").replace(")","")